{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>candidate_mbart_v1</th>\n",
       "      <th>candidate_ul2_v1</th>\n",
       "      <th>candidate_mbart_v2</th>\n",
       "      <th>candidate_ul2_v2</th>\n",
       "      <th>candidate_mbart_v3</th>\n",
       "      <th>candidate_ul2_v3</th>\n",
       "      <th>candidate_mbart_v4</th>\n",
       "      <th>candidate_ul2_v4</th>\n",
       "      <th>candidate_ul2_v3_late</th>\n",
       "      <th>candidate_mbart_v3_late</th>\n",
       "      <th>candidate_mbart_v4_late</th>\n",
       "      <th>candidate_ul2_mc4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mw vanmorgen gedoucht.</td>\n",
       "      <td>mevrouw vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw is vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw is vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw heeft vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw heeft vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw was vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw heeft vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw heeft vanmorgen gedoucht.</td>\n",
       "      <td>Vanmorgen mevrouw gedoucht.</td>\n",
       "      <td>Mevrouw heeft vanmorgen gedoucht.</td>\n",
       "      <td>Mevrouw vanmorgen gedoucht.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alles schoon aan gedaan kon geen schonen BH vi...</td>\n",
       "      <td>Alles schoon aan gedaan kon geen schonen BH vi...</td>\n",
       "      <td>Ik heb haar schone kleren aangedaan alleen kon...</td>\n",
       "      <td>Alles schoon aan gedaan en kon geen schone BH ...</td>\n",
       "      <td>Alles schoon aan gedaan maar kon geen schone B...</td>\n",
       "      <td>Alles schoon aan gedaan, maar kon geen schone ...</td>\n",
       "      <td>Alles schoon aan gedaan, kon geen schone BH vi...</td>\n",
       "      <td>Alles schoon aan gedaan en ik kon geen schone ...</td>\n",
       "      <td>Alles schoon aan gedaan en kon geen schone BH ...</td>\n",
       "      <td>Alles schoon aan gedaan en kon geen schone BH ...</td>\n",
       "      <td>Alles schoon aan gedaan, maar kon geen schone ...</td>\n",
       "      <td>Alles schoon aan gedaan, maar kon geen schone ...</td>\n",
       "      <td>Alles schoon aan gedaan en kon geen schone BH ...</td>\n",
       "      <td>Alles schoon aan gedaan, maar ik kon geen scho...</td>\n",
       "      <td>Alles schoon aan gedaan, kon geen schone BH vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kleding die erhing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die erhing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die ergering heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die erhing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die erhing, heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die erhing heb ik in de wasmand gedaan.</td>\n",
       "      <td>Kleding die erger was heb ik in de wasmand ged...</td>\n",
       "      <td>Kleding die er hing heb ik in de wasmand gedaan.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original   \n",
       "0                             mw vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan kon geen schonen BH vi...   \n",
       "2    Kleding die erhing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                              source   \n",
       "0                        mevrouw vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan kon geen schonen BH vi...   \n",
       "2    Kleding die erhing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                              target   \n",
       "0                     Mevrouw is vanmorgen gedoucht.  \\\n",
       "1  Ik heb haar schone kleren aangedaan alleen kon...   \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                  candidate_mbart_v1   \n",
       "0                     Mevrouw is vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan en kon geen schone BH ...   \n",
       "2  Kleding die ergering heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                    candidate_ul2_v1   \n",
       "0                        Mevrouw vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan maar kon geen schone B...   \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                  candidate_mbart_v2   \n",
       "0                        Mevrouw vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan, maar kon geen schone ...   \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                    candidate_ul2_v2   \n",
       "0                        Mevrouw vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan, kon geen schone BH vi...   \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                  candidate_mbart_v3   \n",
       "0                  Mevrouw heeft vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan en ik kon geen schone ...   \n",
       "2    Kleding die erhing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                    candidate_ul2_v3   \n",
       "0                  Mevrouw heeft vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan en kon geen schone BH ...   \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                  candidate_mbart_v4   \n",
       "0                    Mevrouw was vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan en kon geen schone BH ...   \n",
       "2   Kleding die erhing, heb ik in de wasmand gedaan.   \n",
       "\n",
       "                                    candidate_ul2_v4   \n",
       "0                  Mevrouw heeft vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan, maar kon geen schone ...   \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                               candidate_ul2_v3_late   \n",
       "0                  Mevrouw heeft vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan, maar kon geen schone ...   \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                             candidate_mbart_v3_late   \n",
       "0                        Vanmorgen mevrouw gedoucht.  \\\n",
       "1  Alles schoon aan gedaan en kon geen schone BH ...   \n",
       "2    Kleding die erhing heb ik in de wasmand gedaan.   \n",
       "\n",
       "                             candidate_mbart_v4_late   \n",
       "0                  Mevrouw heeft vanmorgen gedoucht.  \\\n",
       "1  Alles schoon aan gedaan, maar ik kon geen scho...   \n",
       "2  Kleding die erger was heb ik in de wasmand ged...   \n",
       "\n",
       "                                   candidate_ul2_mc4  \n",
       "0                        Mevrouw vanmorgen gedoucht.  \n",
       "1  Alles schoon aan gedaan, kon geen schone BH vi...  \n",
       "2   Kleding die er hing heb ik in de wasmand gedaan.  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from bert_score import score\n",
    "from bert_score import plot_example\n",
    "import sys\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "data_folder_path = os.path.join(current_directory, '..', 'data')\n",
    "data_file_path = os.path.join(data_folder_path, 'candidates\\candidates_annotated_sentences.csv')\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "exclude_columns = ['original', 'source', 'target']\n",
    "\n",
    "df_candidates = df.loc[:, ~df.columns.isin(exclude_columns)]\n",
    "\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def dutch_robbert_evaluation(candidates, references):\n",
    "  # Load model from HuggingFace Hub\n",
    "  tokenizer = AutoTokenizer.from_pretrained('jegorkitskerkin/robbert-v2-dutch-base-mqa-finetuned')\n",
    "  model = AutoModel.from_pretrained('jegorkitskerkin/robbert-v2-dutch-base-mqa-finetuned')\n",
    "\n",
    "  results = []\n",
    "\n",
    "  for i in range(len(candidates)):\n",
    "      # Sentences we want sentence embeddings for\n",
    "      sentences = [references[i], candidates[i]]\n",
    "      encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "      # Compute token embeddings\n",
    "      with torch.no_grad():\n",
    "          model_output = model(**encoded_input)\n",
    "\n",
    "      # Perform pooling. In this case, mean pooling.\n",
    "      sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "      embedding1 = sentence_embeddings[0]\n",
    "      embedding2 = sentence_embeddings[1]\n",
    "\n",
    "      # normalize the embeddings to unit length\n",
    "      embedding1_norm = F.normalize(embedding1, p=2, dim=0)\n",
    "      embedding2_norm = F.normalize(embedding2, p=2, dim=0)\n",
    "\n",
    "      # calculate the cosine similarity between the two embeddings\n",
    "      similarity = torch.dot(embedding1_norm, embedding2_norm)\n",
    "      results.append(float(similarity))\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertscorer(candidates, references, bert_model, num_layers):\n",
    "  \"\"\"\n",
    "  bert_model is any huggingface (dutch) bert_model such as :'textgain/allnli-GroNLP-bert-base-dutch-cased' or 'GroNLP/bert-base-dutch-cased'\n",
    "\n",
    "  num_layers is the bert layer to use for your text embeddings. Optimal layers differ between bert models and you can find more here https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0\n",
    "\n",
    "  default is bert_multilingual which has an optimal num_layers associated with it unlike unchecked bert models.\n",
    "  \"\"\"\n",
    "  # Load the Dutch BERT model and tokenizer\n",
    "  #GroNLP/bert-base-dutch-cased\n",
    "\n",
    "  # Calculate BERTScore\n",
    "  \n",
    "  P, R, F1 = score(candidates, references, model_type=bert_model, num_layers=num_layers)\n",
    "  return [round(x, 3) for x in P.tolist()], [round(x, 3) for x in R.tolist()], [round(x, 3) for x in F1.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu_scores(candidates, references):\n",
    "    bleu_scores_2 = []\n",
    "    bleu_scores_4 = []\n",
    "    weights_2 = [(1./2., 1./2.)]\n",
    "    weights_4 = [(1./4., 1./4., 1./4., 1./4.)]\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        candidate_tokens = candidate.split()  # Assuming whitespace tokenization\n",
    "        reference_tokens = reference.split()  # Assuming whitespace tokenization\n",
    "\n",
    "        # Calculate BLEU score for a single candidate-reference pair\n",
    "        bleu_score_2  = sentence_bleu([reference_tokens], candidate_tokens, weights_2)\n",
    "        bleu_score_4 = sentence_bleu([reference_tokens], candidate_tokens, weights_4)\n",
    "\n",
    "        bleu_scores_2.append(bleu_score_2)\n",
    "        bleu_scores_4.append(bleu_score_4)\n",
    "\n",
    "    return bleu_scores_2, bleu_scores_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge_scores(candidates, references):\n",
    "    rouge = Rouge()\n",
    "\n",
    "    scores = rouge.get_scores(candidates, references)\n",
    "\n",
    "    rouge_1 = []\n",
    "    rouge_2 = []\n",
    "    rouge_l = []\n",
    "    for i in range(len(scores)):\n",
    "      rouge_1.append(scores[i]['rouge-1']['f'])\n",
    "      rouge_2.append(scores[i]['rouge-2']['f'])\n",
    "      rouge_l.append(scores[i]['rouge-l']['f'])\n",
    "    return rouge_1, rouge_2, rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\f.tomassen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\f.tomassen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "    # Tokenize the texts\n",
    "    candidate_tokens = nltk.word_tokenize(candidate)\n",
    "    reference_tokens = nltk.word_tokenize(reference)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_score_value = meteor_score.meteor_score([reference_tokens], candidate_tokens)\n",
    "    return meteor_score_value\n",
    "\n",
    "def compare_texts(candidates, references):\n",
    "    assert len(candidates) == len(references), \"Length of candidate and reference texts must be equal.\"\n",
    "\n",
    "    meteor_scores = []\n",
    "\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        # Calculate METEOR score\n",
    "        meteor_score_value = calculate_meteor(candidate, reference)\n",
    "        meteor_scores.append(meteor_score_value)\n",
    "\n",
    "    return meteor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('E:/Data_exploration/GitHub/paraphraser_code/fine-tuning')\n",
    "from gleu_mine import gleu_calculator_mine\n",
    "def calculate_gleu_scores(baselines, candidates, references, n=4,num_iterations=500):\n",
    "    gleu_scores = []\n",
    "    for i in range(len(candidates)):\n",
    "        gleu_scores.append(float(gleu_calculator_mine(baselines[i], candidates[i], [references[i]], n=n, num_iterations=num_iterations)[0]))\n",
    "    return gleu_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluation(baselines, candidates, targets, index, bert_model='bert-base-multilingual-cased', num_layers=9):\n",
    "  #Precision recall and f1 from bertscore. Baseline score is constant so save it and load it.\n",
    "  P_candidates, R_candidates, F1_candidates = bertscorer(candidates, targets, bert_model, num_layers)\n",
    "  gleu_scores = calculate_gleu_scores(baselines, candidates,targets, 4, 500)\n",
    "  sentence_similarity_robbert = dutch_robbert_evaluation(candidates, targets)\n",
    "\n",
    "  bleu_2, bleu_4 = calculate_bleu_scores(candidates, targets)\n",
    "  rouge_1, rouge_2, rouge_l = calculate_rouge_scores(candidates, targets)\n",
    "  meteor_scores = compare_texts(candidates, targets)\n",
    "  \n",
    "  data = {\n",
    "    \"Source\": baselines,\n",
    "    \"Candidates\": candidates,\n",
    "    \"GLEU+\" : gleu_scores,\n",
    "    \"Targets\": targets,\n",
    "    \"Bertscore score\": F1_candidates,\n",
    "    \"Sentence similarity score\" : sentence_similarity_robbert,\n",
    "    \"Bleu 2\" : bleu_2,\n",
    "    \"Bleu 4\" : bleu_4,\n",
    "    \"Rouge 1\" : rouge_1,\n",
    "    \"Rouge 2\" : rouge_2,\n",
    "    \"Rouge l\" : rouge_l,\n",
    "    \"METEOR\" : meteor_scores\n",
    "  }\n",
    "\n",
    "  extensive_df = pd.DataFrame(data)\n",
    "\n",
    "  data2 = {\n",
    "    \"Average Bertscore score\": sum(F1_candidates)/len(candidates),\n",
    "    \"Average sentence similarity score\" : sum(sentence_similarity_robbert)/len(candidates),\n",
    "    'GLEU' : sum(gleu_scores)/len(gleu_scores),\n",
    "    \"Bleu 2\" : sum(bleu_2)/len(candidates),\n",
    "    #\"Bleu 2 avg difference\" : bleu_2_average,\n",
    "    \"Bleu 4\" : sum(bleu_4)/len(candidates),\n",
    "    \"Rouge 1\" : sum(rouge_1)/len(candidates),\n",
    "    #\"rouge 1 avg difference\" : rouge_1_average,\n",
    "    \"Rouge 2\" : sum(rouge_2)/len(candidates),\n",
    "    #\"rouge 2 avg difference\" : rouge_2_average,\n",
    "    \"Rouge l\" : sum(rouge_l)/len(candidates),\n",
    "    \"METEOR\" : sum(meteor_scores)/len(candidates),\n",
    "    #\"rouge l avg difference\" : rouge_l_average\n",
    "  }\n",
    "\n",
    "  overview_df = pd.DataFrame(data2, index=[index])\n",
    "  \n",
    "\n",
    "  return extensive_df.round(3), overview_df.round(3)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['candidate_mbart_v1', 'candidate_ul2_v1', 'candidate_mbart_v2',\n",
       "       'candidate_ul2_v2', 'candidate_mbart_v3', 'candidate_ul2_v3',\n",
       "       'candidate_mbart_v4', 'candidate_ul2_v4', 'candidate_ul2_v3_late',\n",
       "       'candidate_mbart_v3_late', 'candidate_mbart_v4_late',\n",
       "       'candidate_ul2_mc4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "e:\\Data_exploration\\fine-tune\\310_py\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:2351: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = _a * scale + loc\n",
      "e:\\Data_exploration\\fine-tune\\310_py\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:2352: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = _b * scale + loc\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "e:\\Data_exploration\\fine-tune\\310_py\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "e:\\Data_exploration\\fine-tune\\310_py\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "e:\\Data_exploration\\fine-tune\\310_py\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "sources = list(df['source']) #make_sentence_df(eval_df['train'], 'Preprocessed')\n",
    "targets = list(df['target'])\n",
    "for candidates in df_candidates.columns:\n",
    "    extensive_df, overview_df = evaluation(sources, list(df[candidates]), targets, index=candidates)\n",
    "    df_extension = candidates.replace('candidate_', '')\n",
    "    df_extensive_name = f\"df_extensive_{df_extension}\"\n",
    "    df_overview_name = f\"df_overview_{df_extension}\"\n",
    "\n",
    "    # Update the local namespace (use globals() for the global namespace)\n",
    "    locals()[df_extensive_name] = extensive_df\n",
    "    locals()[df_overview_name] = overview_df\n",
    "\n",
    "    extensive_name = os.path.join(data_folder_path, 'sentence_similarity\\extensive_'+df_extension+'.csv')\n",
    "    overview_name = os.path.join(data_folder_path, 'sentence_similarity\\overview_'+df_extension+'.csv')\n",
    "\n",
    "    extensive_df.to_csv(extensive_name, index=False)\n",
    "    overview_df.to_csv(overview_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Bertscore score</th>\n",
       "      <th>Average sentence similarity score</th>\n",
       "      <th>GLEU</th>\n",
       "      <th>Bleu 2</th>\n",
       "      <th>Bleu 4</th>\n",
       "      <th>Rouge 1</th>\n",
       "      <th>Rouge 2</th>\n",
       "      <th>Rouge l</th>\n",
       "      <th>METEOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>candidate_ul2_v1</th>\n",
       "      <td>0.933</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_ul2_v2</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_ul2_v3</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_ul2_v4</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_mbart_v1</th>\n",
       "      <td>0.933</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_mbart_v2</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_mbart_v3</th>\n",
       "      <td>0.936</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_mbart_v4</th>\n",
       "      <td>0.937</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candidate_ul2_mc4</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Average Bertscore score   \n",
       "candidate_ul2_v1                      0.933  \\\n",
       "candidate_ul2_v2                      0.942   \n",
       "candidate_ul2_v3                      0.938   \n",
       "candidate_ul2_v4                      0.940   \n",
       "candidate_mbart_v1                    0.933   \n",
       "candidate_mbart_v2                    0.934   \n",
       "candidate_mbart_v3                    0.936   \n",
       "candidate_mbart_v4                    0.937   \n",
       "candidate_ul2_mc4                     0.942   \n",
       "\n",
       "                    Average sentence similarity score   GLEU  Bleu 2  Bleu 4   \n",
       "candidate_ul2_v1                                0.933  0.522   0.685   0.531  \\\n",
       "candidate_ul2_v2                                0.945  0.549   0.715   0.551   \n",
       "candidate_ul2_v3                                0.944  0.537   0.695   0.539   \n",
       "candidate_ul2_v4                                0.945  0.541   0.704   0.543   \n",
       "candidate_mbart_v1                              0.930  0.508   0.681   0.522   \n",
       "candidate_mbart_v2                              0.935  0.525   0.683   0.526   \n",
       "candidate_mbart_v3                              0.934  0.512   0.680   0.514   \n",
       "candidate_mbart_v4                              0.942  0.522   0.694   0.519   \n",
       "candidate_ul2_mc4                               0.930  0.539   0.716   0.559   \n",
       "\n",
       "                    Rouge 1  Rouge 2  Rouge l  METEOR  \n",
       "candidate_ul2_v1      0.818    0.685    0.805   0.835  \n",
       "candidate_ul2_v2      0.826    0.699    0.815   0.865  \n",
       "candidate_ul2_v3      0.820    0.685    0.809   0.854  \n",
       "candidate_ul2_v4      0.822    0.688    0.809   0.861  \n",
       "candidate_mbart_v1    0.819    0.680    0.805   0.841  \n",
       "candidate_mbart_v2    0.814    0.673    0.802   0.858  \n",
       "candidate_mbart_v3    0.809    0.668    0.795   0.854  \n",
       "candidate_mbart_v4    0.820    0.681    0.807   0.855  \n",
       "candidate_ul2_mc4     0.827    0.702    0.815   0.870  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [df_overview_ul2_v1, df_overview_ul2_v2, df_overview_ul2_v3, df_overview_ul2_v4, df_overview_mbart_v1, df_overview_mbart_v2, df_overview_mbart_v3, df_overview_mbart_v4, df_overview_ul2_mc4]\n",
    "df_together = pd.concat(dfs)\n",
    "df_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_together.to_csv('overview_all.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
